{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from models.PropertyRegressors import PropertyRegressors\n",
    "from datasets.get_fingerprints import get_fingerprints\n",
    "from datasets.get_molecular_descriptors import get_molecular_descriptors\n",
    "from datasets.get_chembert2as import get_chembert2as\n",
    "from datasets.get_molformers import get_molformers\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "descriptors = get_molecular_descriptors(task='classification').to(device)\n",
    "print(f\"descriptor tensor shape: {descriptors.shape}\")  # [num_samples, 210]\n",
    "\n",
    "fingerprints, label_tensor = get_fingerprints(task='classification')\n",
    "print(f\"fingerprint tensor shape: {fingerprints.shape}\")  # [num_samples, 2048]\n",
    "\n",
    "chembert2as = get_chembert2as(task='classification').to(device)\n",
    "print(f\"chembert2a tensor shape: {chembert2as.shape}\")  # [num_samples, 600]\n",
    "\n",
    "molformers = get_molformers(task='classification').to(device)\n",
    "print(f\"molformers tensor shape: {molformers.shape}\")  # [num_samples, 768]\n",
    "\n",
    "# train(descriptors, fingerprints, chembert2as, molformers, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, descriptors, fingerprints, chembert2as, molformers, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with the different molecular representations\n",
    "        \n",
    "        Args:\n",
    "            descriptors (torch.Tensor): Shape [7807, 210]\n",
    "            fingerprints (torch.Tensor): Shape [7807, 2048]\n",
    "            chembert2as (torch.Tensor): Shape [7807, 600]\n",
    "            molformers (torch.Tensor): Shape [7807, 768]\n",
    "            labels (torch.Tensor): Shape [7807, 1]\n",
    "        \"\"\"\n",
    "        self.descriptors = descriptors\n",
    "        self.fingerprints = fingerprints\n",
    "        self.chembert2as = chembert2as\n",
    "        self.molformers = molformers\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single sample from the dataset\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to return\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing all features and label for the sample\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'descriptors': self.descriptors[idx],\n",
    "            'fingerprints': self.fingerprints[idx],\n",
    "            'chembert2as': self.chembert2as[idx],\n",
    "            'molformers': self.molformers[idx],\n",
    "            'label': self.labels[idx],\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "def create_data_loaders(descriptors, fingerprints, chembert2as, molformers, labels, \n",
    "                       batch_size=32, train_split=0.8, random_seed=42):\n",
    "    \"\"\"\n",
    "    Create train and validation data loaders\n",
    "    \n",
    "    Args:\n",
    "        descriptors (torch.Tensor): Molecular descriptors\n",
    "        fingerprints (torch.Tensor): Molecular fingerprints\n",
    "        chembert2as (torch.Tensor): ChemBERT embeddings\n",
    "        molformers (torch.Tensor): Molformer embeddings\n",
    "        labels (torch.Tensor): Target labels\n",
    "        batch_size (int): Batch size for the data loaders\n",
    "        train_split (float): Proportion of data to use for training\n",
    "        random_seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader)\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    dataset_size = len(labels)\n",
    "    indices = torch.randperm(dataset_size)\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    # Create train dataset\n",
    "    train_dataset = MolecularDataset(\n",
    "        descriptors[train_indices],\n",
    "        fingerprints[train_indices],\n",
    "        chembert2as[train_indices],\n",
    "        molformers[train_indices],\n",
    "        labels[train_indices]\n",
    "    )\n",
    "    \n",
    "    # Create validation dataset\n",
    "    val_dataset = MolecularDataset(\n",
    "        descriptors[val_indices],\n",
    "        fingerprints[val_indices],\n",
    "        chembert2as[val_indices],\n",
    "        molformers[val_indices],\n",
    "        labels[val_indices]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "train_loader, val_loader = create_data_loaders(\n",
    "    descriptors=descriptors,\n",
    "    fingerprints=fingerprints,\n",
    "    chembert2as=chembert2as,\n",
    "    molformers=molformers,\n",
    "    labels=label_tensor,\n",
    "    batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat, reduce, einsum\n",
    "import math\n",
    "from models.DifferentialAttention import DiffAttn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(1, d_model)  # [1, 2048] for broadcasting\n",
    "        position = torch.arange(d_model).unsqueeze(0)  # [1, 2048]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        div_term = div_term[:d_model//2]\n",
    "        \n",
    "        pe[0, 0::2] = torch.sin(position[0, ::2] * div_term)\n",
    "        pe[0, 1::2] = torch.cos(position[0, 1::2] * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)  # [1, 2048]\n",
    "        \n",
    "    def forward(self, x):  # x: [batch_size, 2048]\n",
    "        return x + self.pe  # pe will broadcast across batch dimension\n",
    "\n",
    "class PropertyRegressor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        hid_dim: int, \n",
    "        out_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.diff_attn = DiffAttn(in_dim, hid_dim, depth=0)\n",
    "        self.diff_attn = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            )\n",
    "        self.to_property = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hid_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \n",
    "        x = self.diff_attn(x)\n",
    "        return self.to_property(x)\n",
    "\n",
    "class PropertyRegressors(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hid_dim,\n",
    "        out_dim,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rdkit = PropertyRegressor(210, hid_dim, out_dim)\n",
    "        self.morgan = PropertyRegressor(2048, hid_dim, out_dim)\n",
    "        self.chembert2a = PropertyRegressor(600, hid_dim, out_dim)\n",
    "        self.molformer = PropertyRegressor(768, hid_dim, out_dim)\n",
    "        \n",
    "        self.lambdas = nn.Parameter(torch.ones([4,1]))\n",
    "        \n",
    "        self.rdkit_norm = nn.InstanceNorm1d(210)\n",
    "        # self.morgan_norm = nn.InstanceNorm1d(2048)\n",
    "        self.chembert2a_norm = nn.InstanceNorm1d(600)\n",
    "        self.molformer_norm = nn.InstanceNorm1d(768)\n",
    "        \n",
    "        self.morgan_pe = PositionalEncoding(2048)\n",
    "        self.rdkit_pe = PositionalEncoding(210)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        rdkit: Tensor,\n",
    "        morgan: Tensor,\n",
    "        chembert2a: Tensor,\n",
    "        molformer: Tensor,\n",
    "    ) -> tuple[Tensor, Tensor]:\n",
    "        # print(self.lambdas)\n",
    "        \n",
    "        rdkit = self.rdkit_norm(rdkit)\n",
    "        # morgan = self.morgan_norm(morgan)\n",
    "        chembert2a = self.chembert2a_norm(chembert2a)\n",
    "        molformer = self.molformer_norm(molformer)\n",
    "        \n",
    "        rdkit = self.rdkit_pe(rdkit)\n",
    "        morgan = self.morgan_pe(morgan)\n",
    "        \n",
    "        rdkit_out = self.rdkit(rdkit)\n",
    "        morgan_out = self.morgan(morgan)\n",
    "        chembert2a_out = self.chembert2a(chembert2a)\n",
    "        molformer_out = self.molformer(molformer)\n",
    "        \n",
    "        before_vote = torch.stack([\n",
    "            rdkit_out,\n",
    "            morgan_out,\n",
    "            chembert2a_out,\n",
    "            molformer_out,\n",
    "        ], dim=1)  # shape: [batch_size, 4, out_dim]\n",
    "        \n",
    "        normalized_lambdas = F.softmax(self.lambdas, dim=0)\n",
    "        lambdas = repeat(normalized_lambdas, 'four one -> batch_size four one', batch_size=rdkit.shape[0])\n",
    "        \n",
    "        after_vote = torch.einsum('bfo,bfo->bo', lambdas, before_vote)\n",
    "        \n",
    "        return after_vote.sigmoid(), normalized_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import AUROC\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = PropertyRegressors(128, 1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "epoch_loss = 0.0\n",
    "epoch_val_loss = 0.0\n",
    "\n",
    "auroc = AUROC(task=\"binary\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_aurocs = []\n",
    "val_aurocs = []\n",
    "\n",
    "train_lambdas = []\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        desc = batch['descriptors']          # Shape: [batch_size, 210]\n",
    "        fps = batch['fingerprints']          # Shape: [batch_size, 2048]\n",
    "        chembert = batch['chembert2as']      # Shape: [batch_size, 600]\n",
    "        molformer = batch['molformers']      # Shape: [batch_size, 768]\n",
    "        labels = batch['label']              # Shape: [batch_size]\n",
    "        \n",
    "        # desc = torch.ones_like(desc)\n",
    "        # fps = torch.zeros_like(fps)\n",
    "        # chembert = torch.ones_like(chembert)\n",
    "        # molformer = torch.ones_like(molformer)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions, lambdas = model(desc, fps, chembert, molformer)\n",
    "        # print(predictions)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels.view(-1,1))\n",
    "        auroc(predictions, labels.view(-1,1))\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # Print epoch results\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    report_lambdas = list(lambdas.detach().cpu())\n",
    "    \n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    train_aurocs.append(auroc.compute())\n",
    "    train_lambdas.append(report_lambdas)\n",
    "    # print(f'Epoch {epoch} completed. Average loss: {avg_epoch_loss:.4f}. AUROC: {auroc.compute():.4f}. Lambdas: {report_lambdas}')\n",
    "    epoch_loss = 0.0\n",
    "    auroc.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        last_epoch_val_predictions = torch.tensor([])\n",
    "        last_epoch_val_labels = torch.tensor([])\n",
    "        for batch in val_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            desc = batch['descriptors']          # Shape: [batch_size, 210]\n",
    "            fps = batch['fingerprints']          # Shape: [batch_size, 2048]\n",
    "            chembert = batch['chembert2as']      # Shape: [batch_size, 600]\n",
    "            molformer = batch['molformers']      # Shape: [batch_size, 768]\n",
    "            val_labels = batch['label']              # Shape: [batch_size]\n",
    "            \n",
    "            # desc = torch.ones_like(desc)\n",
    "            # fps = torch.zeros_like(fps)\n",
    "            # chembert = torch.ones_like(chembert)\n",
    "            # molformer = torch.ones_like(molformer)\n",
    "            \n",
    "            # Forward pass\n",
    "            val_predictions, val_lambdas = model(desc, fps, chembert, molformer)\n",
    "            # print(predictions)\n",
    "            # Calculate loss\n",
    "            loss = criterion(val_predictions, val_labels.view(-1,1))\n",
    "            auroc(val_predictions, val_labels.view(-1,1))\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            \n",
    "            epoch_val_loss += loss.item()\n",
    "            last_epoch_val_predictions = torch.cat((last_epoch_val_predictions, val_predictions),dim=0)\n",
    "            last_epoch_val_labels = torch.cat((last_epoch_val_labels, val_labels),dim=0)\n",
    "        # Print epoch results\n",
    "        avg_epoch_loss = epoch_val_loss / len(val_loader)\n",
    "        report_lambdas = list(lambdas.detach().cpu())\n",
    "        # print(f'Epoch {epoch} completed. Average loss: {avg_epoch_loss:.4f}. AUROC: {auroc.compute():.4f}. (Val)')\n",
    "        # print(lambdas)\n",
    "        val_losses.append(avg_epoch_loss)\n",
    "        val_aurocs.append(auroc.compute())\n",
    "        epoch_val_loss = 0.0\n",
    "        auroc.reset()\n",
    "print(lambdas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import AUROC\n",
    "\n",
    "# Reset and compute AUROC\n",
    "print(f\"AUROC: {val_aurocs[-1]:.3f}\")\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "L = last_epoch_val_labels.view(-1,1).detach().cpu().numpy()\n",
    "P = last_epoch_val_predictions.detach().cpu().numpy()\n",
    "\n",
    "# Create box plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([P[L.flatten() == 0].flatten(), P[L.flatten() == 1].flatten()], \n",
    "            labels=['0', '1'])\n",
    "plt.ylabel('Predicted Probabilities')\n",
    "plt.title('Predictions Distribution by Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "best_epoch = np.argmin(val_losses)\n",
    "plt.title(f'best val_loss: Epoch {best_epoch+1}, {val_losses[best_epoch]:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_aurocs, label='train')\n",
    "plt.plot(val_aurocs, label='val')\n",
    "best_epoch = np.argmin(val_losses)\n",
    "plt.title(f'best val_aurocs: Epoch {best_epoch+1}, {val_aurocs[best_epoch]:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert list of lambdas to numpy array for easier plotting\n",
    "lambdas_array = np.array(train_lambdas)  # shape: [n_epochs, 4, 1]\n",
    "lambdas_array = lambdas_array.squeeze()  # shape: [n_epochs, 4]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, len(train_lambdas) + 1)\n",
    "\n",
    "# Plot each lambda\n",
    "Ls = ['RDKit', 'Morgan', 'ChemBERT', 'Molformer']\n",
    "for i in range(4):\n",
    "    plt.plot(epochs, lambdas_array[:, i], label=Ls[i], marker='o')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Lambda Values')\n",
    "plt.title(f'Evolution of Lambda Values During Training\\nRatio at the best performance: Epoch {best_epoch+1};\\n{lambdas_array[best_epoch]}')\n",
    "plt.axvline(best_epoch+1)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(labels, predictions):\n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "plot_roc_curve(\n",
    "    val_labels.view(-1,1).detach().cpu().numpy(),\n",
    "    val_predictions.detach().cpu().numpy()\n",
    ")\n",
    "\n",
    "plot_roc_curve(\n",
    "    labels.view(-1,1).detach().cpu().numpy(),\n",
    "    predictions.detach().cpu().numpy()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midexam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
